{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iEdoboZGvbAL"
      },
      "outputs": [],
      "source": [
        "! pip install pycocotools scikit-image tensorboard\n",
        "! pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_utils\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "from skimage import draw as sk_draw\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "import os\n",
        "import contextlib\n",
        "import io\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN0ROFtsvbAN",
        "outputId": "a129bfdd-394a-4f25-dc73-d929d582ab0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/junior/Documents/stage-4a-SEDOGBO/model/model1/semi_supervised\n"
          ]
        }
      ],
      "source": [
        "HOME = os.getcwd()\n",
        "LOG_DIR=\"metrics/\"\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "olfkaWoBvbAP"
      },
      "outputs": [],
      "source": [
        "#! pip uninstall -y torch torchvision\n",
        "! pip install torch>=2.0.0 torchvision>=0.10.0\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models.segmentation\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2o5H6xY-vbAP"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "rf = Roboflow(api_key=\"K72bvIl0rTcvckcth1sm\")\n",
        "#project = rf.workspace(\"insa-3ptmt\").project(\"simplified\")\n",
        "#version = project.version(2)\n",
        "\n",
        "#project = rf.workspace(\"insa-3ptmt\").project(\"deep_forest\")\n",
        "#version = project.version(1)\n",
        "\n",
        "project = rf.workspace(\"insa-3ptmt\").project(\"final-tree-detection\")\n",
        "version = project.version(5)\n",
        "\n",
        "dataset = version.download(\"coco-segmentation\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "axWazUeMvbAQ"
      },
      "outputs": [],
      "source": [
        "class CustomCOCODataset(Dataset):\n",
        "    def __init__(self, coco_json_path, transform, image_dir, imageSize : list[int]):\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            self.coco = COCO(coco_json_path)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_ids = list(self.coco.imgs.keys())\n",
        "        self.imageSize = imageSize\n",
        "        self.annFile = coco_json_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)\n",
        "        assert len(img_info) == 1, f\"Plus d'une annotation pour l'image {img_id}\"\n",
        "        img_info = img_info[0]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "        image = cv2.imread(img_path) #Image.open(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, self.imageSize, cv2.INTER_LINEAR)\n",
        "        image = torch.as_tensor(image, dtype=torch.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        ann_ids = self.coco.getAnnIds(img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "        num_objects = len(anns)\n",
        "        #print(\"Image\", img_path, \"avec id = \", img_id, \"comporte \", len(anns), \" annotations\")\n",
        "        masks = np.zeros((num_objects, self.imageSize[0], self.imageSize[1]), dtype=np.uint8)\n",
        "        all_bboxes = torch.zeros([num_objects,4], dtype=torch.float32)\n",
        "        object_number = 1\n",
        "        for index, ann in enumerate(anns):\n",
        "            x_min, y_min, width, height = ann['bbox']\n",
        "            normalized_bbox = (x_min, y_min, x_min + width, y_min + height)\n",
        "            all_bboxes[index] = torch.tensor(normalized_bbox)\n",
        "            for seg in ann['segmentation']:\n",
        "                rr, cc = sk_draw.polygon(seg[1::2], seg[0::2], masks[index].shape)\n",
        "                masks[index, rr, cc] = min(object_number, 254)\n",
        "                object_number += 1\n",
        "\n",
        "        image = image.swapaxes(0, 2).swapaxes(1, 2)\n",
        "        masks = (masks > 0).astype(bool)\n",
        "        data = {\"boxes\": all_bboxes, \"labels\": torch.ones((num_objects, ), dtype=torch.int64),\n",
        "                \"masks\": masks, \"image_id\": img_id}\n",
        "        #print(\"Mask -- shape : \", masks.shape, \"minimum = \", torch.min(torch.tensor(masks)).item(), 'maximum = ', torch.max(torch.tensor(masks)).item())\n",
        "        #print(\"Original image -- shape : \", image.shape, \"minimum = \", torch.min(image).item(), 'maximum = ', torch.max(image).item())\n",
        "\n",
        "        return image, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nx3M1lJrvbAR"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"final-tree-detection-5/\"\n",
        "train_coco_json_path = DATASET_PATH+'train/_annotations.coco.json'\n",
        "valid_coco_json_path = DATASET_PATH+'valid/_annotations.coco.json'\n",
        "test_coco_json_path = DATASET_PATH+'test/_annotations.coco.json'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iCtDYDPvbAS",
        "outputId": "39e30c01-1613-4dc6-c316-b6c26ff07c19"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for sample in batch:\n",
        "        images.append(sample[0])\n",
        "        targets.append(sample[1])\n",
        "\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    return images, targets\n",
        "imageSize = [1024, 1024]\n",
        "train_dataset = CustomCOCODataset(train_coco_json_path, transform, DATASET_PATH+\"train\", imageSize = imageSize)\n",
        "valid_dataset = CustomCOCODataset(valid_coco_json_path, transform, DATASET_PATH+\"valid\", imageSize = imageSize)\n",
        "test_dataset = CustomCOCODataset(test_coco_json_path, transform, DATASET_PATH+\"test\", imageSize = imageSize)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, drop_last=False, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "#clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTauwQQdvbAT",
        "outputId": "0b1322f8-c138-4a33-bacf-8d1991b10b01"
      },
      "outputs": [],
      "source": [
        "def show_mask(random_color=False):\n",
        "    images, targets = next(iter(valid_dataloader))\n",
        "    print(\"Images batch shape = \", images.shape)\n",
        "    image, target = images[0], targets[0]\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.grid(False)\n",
        "        ax.axis('off')\n",
        "    bg = image.permute(1, 2, 0).cpu().numpy()\n",
        "    bg = (bg * 255.0).astype(np.uint8)\n",
        "    msk = bg.copy()\n",
        "\n",
        "    axes[0].imshow(bg)\n",
        "    axes[1].imshow(msk)\n",
        "    ground_truth_seg = np.array(target[\"masks\"])\n",
        "    h, w = ground_truth_seg.shape[-2:]\n",
        "    for mask in ground_truth_seg:\n",
        "        if random_color:\n",
        "            color = np.concatenate([np.random.random(3), np.array([0.3])], axis=0)\n",
        "        else:\n",
        "            color = np.array([30/255, 144/255, 255/255, 0.5])\n",
        "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "        ax.imshow(mask_image)\n",
        "\n",
        "#show_mask(random_color=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Za595SHvgSFB"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "def calculate_iou(mask1, mask2):\n",
        "    intersection = np.logical_and(mask1, mask2)\n",
        "    union = np.logical_or(mask1, mask2)\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "    return iou\n",
        "\n",
        "def calculate_iou_with_bbox(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x1g, y1g, x2g, y2g = box2\n",
        "    xi1 = max(x1, x1g)\n",
        "    yi1 = max(y1, y1g)\n",
        "    xi2 = min(x2, x2g)\n",
        "    yi2 = min(y2, y2g)\n",
        "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "    iou = inter_area / union_area\n",
        "    return iou\n",
        "\n",
        "def match_masks(pred_masks, gt_masks, iou_threshold=0.5):\n",
        "    num_pred = len(pred_masks)\n",
        "    num_gt = len(gt_masks)\n",
        "    iou_matrix = np.zeros((num_gt, num_pred))\n",
        "    for i in range(num_gt):\n",
        "        for j in range(num_pred):\n",
        "            iou_matrix[i, j] = calculate_iou(gt_masks[i], pred_masks[j])\n",
        "    iou_matrix[iou_matrix < iou_threshold] = 0\n",
        "    row_indices, col_indices = linear_sum_assignment(-iou_matrix)\n",
        "    correspondances = []\n",
        "    used_pred_indices = set()\n",
        "    for gt_idx, pred_idx in zip(row_indices, col_indices):\n",
        "        if iou_matrix[gt_idx, pred_idx] >= iou_threshold:\n",
        "            correspondances.append((gt_idx, pred_idx, iou_matrix[gt_idx, pred_idx]))\n",
        "            used_pred_indices.add(pred_idx)\n",
        "\n",
        "    return correspondances\n",
        "\n",
        "def precision_score(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask * groundtruth_mask)\n",
        "    total_pixel_pred = np.sum(pred_mask)\n",
        "    if total_pixel_pred == 0:\n",
        "        return 0.0\n",
        "    precision = intersect / total_pixel_pred\n",
        "    return round(precision, 3)\n",
        "\n",
        "def recall_score(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask * groundtruth_mask)\n",
        "    total_pixel_truth = np.sum(groundtruth_mask)\n",
        "    if total_pixel_truth == 0:\n",
        "        return 0.0\n",
        "    recall = intersect / total_pixel_truth\n",
        "    return round(recall, 3)\n",
        "\n",
        "def calculate_precision_recall(pred_masks, gt_masks, iou_threshold=0.5, confidence=0.5):\n",
        "    #print(\"Nombres de masques : --prédits = \", len(pred_masks),\" --vrais = \", len(gt_masks))\n",
        "    pred_masks = pred_masks >= confidence\n",
        "    gt_masks = gt_masks >= confidence\n",
        "    correspondances = match_masks(pred_masks, gt_masks, iou_threshold)\n",
        "    avg_precision, avg_recall = [], []\n",
        "\n",
        "    matched_gt_indices = set([corr[0] for corr in correspondances])\n",
        "    matched_pred_indices = set([corr[1] for corr in correspondances])\n",
        "\n",
        "    for gt_idx, pred_idx, _ in correspondances:\n",
        "        mask_precision = precision_score(gt_masks[gt_idx], pred_masks[pred_idx])\n",
        "        avg_precision.append(mask_precision)\n",
        "        mask_recall = recall_score(gt_masks[gt_idx], pred_masks[pred_idx])\n",
        "        avg_recall.append(mask_recall)\n",
        "\n",
        "    # Calculer les faux positifs (FP) et les faux négatifs (FN)\n",
        "    fp_count = len(pred_masks) - len(matched_pred_indices)\n",
        "    fn_count = len(gt_masks) - len(matched_gt_indices)\n",
        "    #print(f\"Nombre de faux positifs = {fp_count} et nombre de faux négatifs = {fn_count}\")\n",
        "    # Ajouter des précisions/rappels de 0 pour chaque faux positif et faux négatif\n",
        "    avg_precision.extend([0] * fp_count)\n",
        "    avg_recall.extend([0] * fn_count)\n",
        "    \n",
        "    if len(avg_precision) == 0:\n",
        "        avg_precision_value = 0\n",
        "    else:\n",
        "        avg_precision_value = sum(avg_precision) / len(avg_precision)\n",
        "    \n",
        "    if len(avg_recall) == 0:\n",
        "        avg_recall_value = 0\n",
        "    else:\n",
        "        avg_recall_value = sum(avg_recall) / len(avg_recall)\n",
        "\n",
        "    return avg_precision_value, avg_recall_value, correspondances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Précision: 1.000\n",
            "Rappel: 0.500\n"
          ]
        }
      ],
      "source": [
        "gt_masks = np.array([\n",
        "    [[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], \n",
        "    [[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
        "])\n",
        "\n",
        "pred_masks = np.array([\n",
        "    [[0.9, 0.8, 0.1, 0.0], [0.9, 0.8, 0.1, 0.0], [0.2, 0.2, 0.1, 0.0], [0.2, 0.2, 0.1, 0.0]]\n",
        "])\n",
        "\n",
        "precision, recall, _ = calculate_precision_recall(pred_masks, gt_masks, iou_threshold=0.5, confidence=0.5)\n",
        "\n",
        "print(f'Précision: {precision:.3f}')\n",
        "print(f'Rappel: {recall:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "46RQI4Q9bdiW"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device, iou_threshold = 0.5, confidence=0.5):\n",
        "    # Evaluate the model over a dataset and compute/log metrics\n",
        "    # This function does is composed of 2 main parts\n",
        "    #===================================================================#\n",
        "    # getting losses infos #\n",
        "    #===================================================================#\n",
        "    model.train()\n",
        "    batch_losses = {\"val_box_loss\": [], \"val_seg_loss\": [], \"val_cls_loss\": [], \"val_global\": []}\n",
        "    with torch.no_grad():\n",
        "        for val_batch in valid_dataloader:\n",
        "            images, targets = val_batch\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets=[{k: torch.as_tensor(v).to(device) for k,v in t.items()} for t in targets]\n",
        "            val_loss_dict = model(images, targets)\n",
        "            val_losses = sum(loss for loss in val_loss_dict.values())\n",
        "            # Model output keys : loss_classifier, loss_box_reg, loss_mask, loss_objectness, loss_rpn_box_reg\n",
        "            batch_losses[\"val_box_loss\"].append(val_loss_dict[\"loss_box_reg\"].item())\n",
        "            batch_losses[\"val_seg_loss\"].append(val_loss_dict[\"loss_mask\"].item())\n",
        "            batch_losses[\"val_cls_loss\"].append(val_loss_dict[\"loss_classifier\"].item())\n",
        "            batch_losses[\"val_global\"].append(val_losses.item())\n",
        "\n",
        "    for k in batch_losses.keys():\n",
        "        batch_losses[k] = mean(batch_losses[k])\n",
        "\n",
        "    #===================================================================#\n",
        "    # getting metrics #\n",
        "    #===================================================================#\n",
        "    model.eval()\n",
        "    with contextlib.redirect_stdout(io.StringIO()):\n",
        "        coco = COCO(data_loader.dataset.annFile)\n",
        "    coco_results = []\n",
        "    all_ious = []\n",
        "    precision_epoch, recall_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets=[{k: torch.as_tensor(v).to(device) for k,v in t.items()} for t in targets]\n",
        "            outputs = model(images)\n",
        "            #print(\"predictions masks : \", outputs[0][\"masks\"].shape)\n",
        "            for i, output in enumerate(outputs):\n",
        "                boxes = output['boxes'].cpu().numpy()\n",
        "                scores = output['scores'].cpu().numpy()\n",
        "                labels = output['labels'].cpu().numpy()\n",
        "                masks = output['masks'].cpu().numpy()\n",
        "                precision_one_img, recall_one_img, _ = calculate_precision_recall(masks, targets[i]['masks'].cpu().numpy(), \n",
        "                                                        iou_threshold=iou_threshold, confidence=confidence)\n",
        "                \n",
        "                precision_epoch.append(precision_one_img)\n",
        "                recall_epoch.append(recall_one_img)\n",
        "                gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "                #print(\"Nombre de bbox prédites : \", len(boxes))\n",
        "                for j in range(len(boxes)):\n",
        "                    box = boxes[j]\n",
        "                    score = scores[j]\n",
        "                    label = labels[j]\n",
        "                    mask = masks[j][0]\n",
        "                    #print(\"Mask for box : \", mask.shape, mask)\n",
        "                    mask = (mask * 255).astype(np.uint8)\n",
        "                    ious = [calculate_iou_with_bbox(box, gt_box) for gt_box in gt_boxes]\n",
        "                    if ious:\n",
        "                        max_iou = max(ious)\n",
        "                        all_ious.append(max_iou)\n",
        "\n",
        "                    coco_results.append({\n",
        "                        'image_id': targets[i][\"image_id\"].item(),\n",
        "                        'category_id': int(label),\n",
        "                        'bbox': box.tolist(),\n",
        "                        'score': float(score),\n",
        "                        'segmentation': mask_utils.encode(np.asfortranarray(mask))\n",
        "                    })\n",
        "\n",
        "    precision_epoch = sum(precision_epoch) / len(precision_epoch)\n",
        "    recall_epoch = sum(recall_epoch) / len(recall_epoch)\n",
        "    print(\"Precision = \", precision_epoch, \" Recall = \", recall_epoch)\n",
        "    # COCO metrics evaluation\n",
        "    #with contextlib.redirect_stdout(io.StringIO()):\n",
        "    #    coco_dt = coco.loadRes(coco_results)\n",
        "    #    coco_eval = COCOeval(coco, coco_dt, iouType='segm')\n",
        "    #    coco_eval.evaluate()\n",
        "    #    coco_eval.accumulate()\n",
        "    #    coco_eval.summarize()\n",
        "    # IoU computation\n",
        "    mean_iou = np.mean(all_ious)\n",
        "    #metrics = {\"mAP\": coco_eval.stats[0], \"AP50\": coco_eval.stats[1], \"IoU\": mean_iou}\n",
        "    metrics = {\"precision\": precision_epoch, \"recall\": recall_epoch, \"IoU\": mean_iou}\n",
        "    for k in batch_losses.keys():\n",
        "        metrics[k] = batch_losses[k]\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "STk3qUF7vbAW"
      },
      "outputs": [],
      "source": [
        "def fine_tune(model, optimizer, writer, num_epochs, train_dataloader, valid_dataloader = None):\n",
        "    num_steps = len(train_dataloader)\n",
        "    for epoch in range(num_epochs):\n",
        "        batch_losses = {\"box_loss\": [], \"seg_loss\": [], \"cls_loss\": [], \"global\": []}\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        for index, train_batch in enumerate((pbatch := tqdm(train_dataloader, colour='green'))):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            images, targets = train_batch\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets=[{k: torch.as_tensor(v).to(device) for k,v in t.items()} for t in targets]\n",
        "            loss_dict = model(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            #Model output keys : loss_classifier, loss_box_reg, loss_mask, loss_objectness, loss_rpn_box_reg\n",
        "            batch_losses[\"box_loss\"].append(loss_dict[\"loss_box_reg\"].item())\n",
        "            batch_losses[\"seg_loss\"].append(loss_dict[\"loss_mask\"].item())\n",
        "            batch_losses[\"cls_loss\"].append(loss_dict[\"loss_classifier\"].item())\n",
        "            batch_losses[\"global\"].append(losses.item())\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if index == num_steps-1:\n",
        "                for k in batch_losses.keys():\n",
        "                    batch_losses[k] = mean(batch_losses[k])\n",
        "                    writer.add_scalar(k, batch_losses[k], epoch)\n",
        "                if valid_dataloader is not None:\n",
        "                    #Validation loop\n",
        "                    metrics = evaluate(model, valid_dataloader, device)\n",
        "                    for k in metrics.keys():\n",
        "                        writer.add_scalar(k, metrics[k], epoch)\n",
        "\n",
        "    writer.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P39jvYZnvbAU",
        "outputId": "896090ae-6c37-4078-95eb-bb5c426e83e3"
      },
      "outputs": [],
      "source": [
        "model=torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"MaskRCNN_ResNet50_FPN_Weights.COCO_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lAA8LYy0vbAV"
      },
      "outputs": [],
      "source": [
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor=FastRCNNPredictor(in_features,num_classes=2)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
        "writer = SummaryWriter(log_dir=LOG_DIR)\n",
        "model.to(device)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1fnaQXKScEeS",
        "outputId": "84c55a05-3b60-43c1-d90d-9746cf0865af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m██████████\u001b[0m| 17/17 [02:24<00:00,  8.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision =  0.1812475  Recall =  0.06391744791666668\n",
            "Epoch [2/2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[32m██████████\u001b[0m| 17/17 [02:25<00:00,  8.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision =  0.1881125  Recall =  0.06860213541666667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 2\n",
        "fine_tune(model, optimizer, writer, num_epochs, train_dataloader, valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqrrVjNmvbAW"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "torch.save(model.state_dict(), f\"model{num_epochs}.torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8wsJtVbyAKF"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=LOG_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiBYMt1jvbAX"
      },
      "outputs": [],
      "source": [
        "def draw_metrics(metrics : dict):\n",
        "    sns.set_theme(style=\"darkgrid\")\n",
        "    ncols, nepochs = len(metrics), len(next(iter(metrics.values())))\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=ncols, figsize=(10, 3))\n",
        "    if ncols == 1:\n",
        "        axs = [axs]\n",
        "    for idx, (metric, values) in enumerate(metrics.items()):\n",
        "        ax = axs[idx]\n",
        "        sns.lineplot(x=np.arange(nepochs), y=values, ax=ax, marker='o', linestyle='-', color=\"blue\")\n",
        "        ax.set_xlabel(metric.capitalize())\n",
        "        ax.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "draw_metrics({\"box_loss\": metrics[\"box_loss\"], \"seg_loss\": metrics[\"seg_loss\"], \"cls_loss\": metrics[\"cls_loss\"], \"global\": metrics[\"global\"]})\n",
        "draw_metrics({\"val_box_loss\": metrics[\"val_box_loss\"], \"val_seg_loss\": metrics[\"val_seg_loss\"], \"val_cls_loss\": metrics[\"val_cls_loss\"], \"val_global\": metrics[\"val_global\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG4mAZwGvbAX"
      },
      "outputs": [],
      "source": [
        "! mkdir runs\n",
        "SAVES_PATH = \"runs/\"\n",
        "def visual_inference(image, prediction: dict, batch_index : int, confidence_threshold : float = 0.8,\n",
        "                     prediction_path: str = None):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.grid(False)\n",
        "        ax.axis('off')\n",
        "    bg = image.permute(1, 2, 0).detach().cpu().numpy()\n",
        "    bg = (bg * 255.0).astype(np.uint8)\n",
        "    seg_im = bg.copy()\n",
        "    for i in range(len(prediction['masks'])):\n",
        "        msk=prediction['masks'][i,0].detach().cpu().numpy()\n",
        "        scr=prediction['scores'][i].detach().cpu().numpy()\n",
        "        if scr>confidence_threshold:\n",
        "            seg_im[msk >= 0.5, 0] = 100\n",
        "            seg_im[msk >= 0.5, 1] = 151\n",
        "            seg_im[msk >= 0.5, 2] = 177\n",
        "    axs[0].imshow(bg)\n",
        "    axs[1].imshow(seg_im)\n",
        "    #show_mask(prediction['masks'].detach().cpu().numpy(), axs[1], True)\n",
        "    if not prediction_path:\n",
        "      fig.savefig(SAVES_PATH+f'batch_{batch_index}.png', bbox_inches='tight')\n",
        "    else:\n",
        "      fig.savefig(SAVES_PATH + prediction_path, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfqnxLIqvbAY"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "precision, recall = 0.0, 0.0\n",
        "with torch.no_grad():\n",
        "    for index, (images, targets) in enumerate((pbatch := tqdm(test_dataloader, colour='green'))):\n",
        "        predictions = model(images.to(device))\n",
        "        #pred_masks = [m[\"masks\"].detach().cpu().numpy() for m in predictions]\n",
        "        #true_masks = [t[\"masks\"] for t in  targets]\n",
        "        #for target, prediction in zip(targets, predictions):\n",
        "        #    compute_metrics(predictions, targets)\n",
        "        # model output : boxes, labels, scores, masks\n",
        "        for i in range(len(images)):\n",
        "            visual_inference(images[i], predictions[i], index, confidence_threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZb7M8hTkA4V"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def predict_non_annotated_image(file_id:str):\n",
        "  file_path = f\"data/{file_id}.jpeg\"\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      image = cv2.imread(file_path) #Image.open(img_path)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      image = cv2.resize(image, imageSize, cv2.INTER_LINEAR)\n",
        "      image = torch.as_tensor(image, dtype=torch.float32)\n",
        "      image /= 255.0\n",
        "      image = image.swapaxes(0, 2).swapaxes(1, 2)\n",
        "      uns_image = torch.unsqueeze(image, 0)\n",
        "      prediction = model(uns_image.to(device))\n",
        "      visual_inference(image, prediction[0], random.randint(50, 100), confidence_threshold=0.2, prediction_path=file_id+\".png\")\n",
        "\n",
        "files = [\"0_17\", \"0_6\", \"1_19\", \"1_20\", \"3_10\", \"2_15\", \"1_17\"]\n",
        "for file in files:\n",
        "  predict_non_annotated_image(file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
